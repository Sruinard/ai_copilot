{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, n_heads) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dims = embed_size // n_heads\n",
    "\n",
    "        assert embed_size % n_heads == 0\n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size)\n",
    "        self.keys = nn.Linear(embed_size, embed_size)\n",
    "        self.queries = nn.Linear(embed_size, embed_size)\n",
    "        self.fc = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "        \n",
    "    def forward(self, values, keys, queries, mask=None):\n",
    "        # values.shape = [batch_size, seq_len, embed_dim]\n",
    "        N = values.shape[0]\n",
    "        values_len = values.shape[1]\n",
    "        keys_len = keys.shape[1]\n",
    "        queries_len = queries.shape[1]\n",
    "\n",
    "        values = values.reshape(N, values_len, self.n_heads, self.head_dims)\n",
    "        keys = keys.reshape(N, keys_len, self.n_heads, self.head_dims)\n",
    "        queries = queries.reshape(N, queries_len, self.n_heads, self.head_dims)\n",
    "\n",
    "        x = self.scaled_dot_product(values, keys, queries, mask)\n",
    "        return self.fc(x)\n",
    "\n",
    "        \n",
    "\n",
    "    def scaled_dot_product(self, values, keys, queries, mask):\n",
    "        N = values.shape[0]\n",
    "        queries_len = queries.shape[1]\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy.masked_fill(mask == 0, float('-1e20'))\n",
    "        \n",
    "        attention = F.softmax((energy / (self.embed_size ** 0.5)), dim=3)\n",
    "\n",
    "        out = torch.einsum('nhqk,nvhd->nqhd', [attention, values]).reshape(N, queries_len, self.n_heads * self.head_dims)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, n_heads, drop_out, forward_expension) -> None:\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(embed_size, n_heads)\n",
    "        self.norm_1 = nn.LayerNorm(embed_size)\n",
    "        self.norm_2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expension * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expension * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        attenion = self.multi_head_attention(values, keys, queries, mask)\n",
    "        x = self.dropout(self.norm_1(attenion + queries))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm_2(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, embed_size, n_layers, n_heads, forward_expansion, dropout, device, max_length\n",
    "        )  -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
    "        self.layers = nn.ModuleList([\n",
    "                TransformerBlock(embed_size, n_heads, drop_out=dropout, forward_expension=forward_expansion) \n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, sequence_length = x.shape\n",
    "        positions = torch.arange(0, sequence_length).expand(N, sequence_length).to(self.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.positional_embedding(positions))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x, mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, n_heads, forward_expension, dropout, device) -> None:\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, n_heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, n_heads, dropout, forward_expension\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, values, keys, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        queries = self.dropout(self.norm(attention + x))\n",
    "\n",
    "        out = self.transformer_block(values, keys, queries, src_mask)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, trg_vocab_size, embed_size, num_layers, n_heads, forward_expansion, dropout, device, max_length) -> None:\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.positional_embeddings = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(embed_size, n_heads, forward_expansion, dropout, device)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, sequence_length = x.shape[0], x.shape[1]\n",
    "        positions = torch.arange(0, sequence_length).expand(N, sequence_length).to(device=self.device)\n",
    "        x = self.dropout(self.word_embeddings(x) + self.positional_embeddings(positions))\n",
    "        \n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, device='cuda', max_length=100) -> None:\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length)\n",
    "        self.decoder = Decoder(trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length)\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        pred = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return pred\n",
    "\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones(trg_len, trg_len)).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "         3, 3, 3, 3, 3]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "x = torch.tensor([[1, 2, 3, 4, 5, 6]]).to(device)\n",
    "y = torch.tensor([[4, 1, 3, 9, 5, 8]]).to(device)\n",
    "\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "src_vocab_size = 8\n",
    "trg_vocab_size = 10\n",
    "\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n",
    "\n",
    "out = model(x, y[:, :1])\n",
    "probs, indx = torch.max(out, axis=-1)\n",
    "translation = torch.tensor([[1, 2]])\n",
    "for _ in range(99):\n",
    "    out = model(x, translation)\n",
    "    probs, indx = torch.max(out, axis=-1)\n",
    "    translation = torch.concat([translation[0], indx[0, -1:]]).unsqueeze(0)\n",
    "    # translation = torch.concat([translation[0], indx[0, -1:]]).unsqueeze(0)\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "filepath = \"../data/python/final/jsonl/train/python_train_0.jsonl\"\n",
    "with open(filepath) as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('def predict(X_img_path, knn_clf=None, model_path=None, '\n",
      " 'distance_threshold=0.6):\\n'\n",
      " '    \"\"\"\\n'\n",
      " '    Recognizes faces in given image using a trained KNN classifier\\n'\n",
      " '\\n'\n",
      " '    :param X_img_path: path to image to be recognized\\n'\n",
      " '    :param knn_clf: (optional) a knn classifier object. if not specified, '\n",
      " 'model_save_path must be specified.\\n'\n",
      " '    :param model_path: (optional) path to a pickled knn classifier. if not '\n",
      " 'specified, model_save_path must be knn_clf.\\n'\n",
      " '    :param distance_threshold: (optional) distance threshold for face '\n",
      " 'classification. the larger it is, the more chance\\n'\n",
      " '           of mis-classifying an unknown person as a known one.\\n'\n",
      " '    :return: a list of names and face locations for the recognized faces in '\n",
      " 'the image: [(name, bounding box), ...].\\n'\n",
      " \"        For faces of unrecognized persons, the name 'unknown' will be \"\n",
      " 'returned.\\n'\n",
      " '    \"\"\"\\n'\n",
      " '    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] '\n",
      " 'not in ALLOWED_EXTENSIONS:\\n'\n",
      " '        raise Exception(\"Invalid image path: {}\".format(X_img_path))\\n'\n",
      " '\\n'\n",
      " '    if knn_clf is None and model_path is None:\\n'\n",
      " '        raise Exception(\"Must supply knn classifier either thourgh knn_clf '\n",
      " 'or model_path\")\\n'\n",
      " '\\n'\n",
      " '    # Load a trained KNN model (if one was passed in)\\n'\n",
      " '    if knn_clf is None:\\n'\n",
      " \"        with open(model_path, 'rb') as f:\\n\"\n",
      " '            knn_clf = pickle.load(f)\\n'\n",
      " '\\n'\n",
      " '    # Load image file and find face locations\\n'\n",
      " '    X_img = face_recognition.load_image_file(X_img_path)\\n'\n",
      " '    X_face_locations = face_recognition.face_locations(X_img)\\n'\n",
      " '\\n'\n",
      " '    # If no faces are found in the image, return an empty result.\\n'\n",
      " '    if len(X_face_locations) == 0:\\n'\n",
      " '        return []\\n'\n",
      " '\\n'\n",
      " '    # Find encodings for faces in the test iamge\\n'\n",
      " '    faces_encodings = face_recognition.face_encodings(X_img, '\n",
      " 'known_face_locations=X_face_locations)\\n'\n",
      " '\\n'\n",
      " '    # Use the KNN model to find the best matches for the test face\\n'\n",
      " '    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)\\n'\n",
      " '    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in '\n",
      " 'range(len(X_face_locations))]\\n'\n",
      " '\\n'\n",
      " \"    # Predict classes and remove classifications that aren't within the \"\n",
      " 'threshold\\n'\n",
      " '    return [(pred, loc) if rec else (\"unknown\", loc) for pred, loc, rec in '\n",
      " 'zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(\n",
    "data[1][\"code\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage valid: 0.96\n"
     ]
    }
   ],
   "source": [
    "valid_samples = 0\n",
    "invalid_samples = 0\n",
    "for row in data:\n",
    "    func_length = len(row[\"code\"].split('\"\"\"'))\n",
    "    func_length_single_quote = len(row[\"code\"].split(\"'''\"))\n",
    "    if func_length == 3 or func_length_single_quote == 3:\n",
    "        valid_samples += 1\n",
    "        val_example = row\n",
    "    else:\n",
    "        invalid_samples +=1\n",
    "        inval_example = row\n",
    "print(f\"Percentage valid: {valid_samples / (valid_samples + invalid_samples)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 0.7726,  0.1959, -0.2742, -0.4414, -0.1205,  0.5896,  0.2676,\n",
      "           0.2441,  1.3759, -0.0921],\n",
      "         [-0.7718, -1.2759, -0.1908,  0.4687,  0.5650,  0.2740,  0.0949,\n",
      "           0.0123,  1.1677,  0.1628],\n",
      "         [ 0.2920, -1.5287, -1.2610, -0.3102,  0.0399, -0.5319,  0.2206,\n",
      "          -0.8736,  0.2447, -0.4729],\n",
      "         [ 0.4001,  0.0745,  0.8375, -0.1703, -1.0116, -1.2133,  1.6221,\n",
      "          -0.9419, -1.3988,  0.0463],\n",
      "         [ 0.9865, -0.9952,  0.4861, -0.0090,  0.9086, -0.1690,  0.3133,\n",
      "           1.3860, -0.1374, -0.4883],\n",
      "         [ 1.0023, -1.8284,  0.3466,  1.0263, -1.5314, -1.4405,  1.4825,\n",
      "          -0.6440, -0.5110, -0.3031],\n",
      "         [-0.9094,  1.4731, -0.1728, -0.2754,  1.6843,  0.8862,  0.2473,\n",
      "           0.0916, -0.3818,  1.0779],\n",
      "         [ 0.6954, -0.9430, -0.3950,  0.9595, -1.8052,  0.6318,  1.0113,\n",
      "          -0.1440, -1.6109, -0.8765],\n",
      "         [-0.4137, -0.9999, -0.5800,  2.2038,  0.7193, -0.5238, -0.2172,\n",
      "           0.6618, -0.4866,  0.1218],\n",
      "         [-0.1828,  2.5232,  0.7855,  1.2489,  1.0359, -1.2540, -1.5563,\n",
      "          -1.0133,  0.7650, -0.3926],\n",
      "         [-1.8902, -0.8437,  2.4591,  0.3986,  2.0114,  0.5541,  1.2676,\n",
      "          -0.0705, -0.2490,  0.7999],\n",
      "         [ 0.0619, -1.0068,  0.9872,  0.1820,  1.8051, -0.6784, -0.1133,\n",
      "           0.4186, -0.0151, -0.3995]]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7726,  0.1959, -0.2742, -0.4414, -0.1205,  0.5896,  0.2676,\n",
       "           0.2441,  1.3759, -0.0921],\n",
       "         [-0.7718, -1.2759, -0.1908,  0.4687,  0.5650,  0.2740,  0.0949,\n",
       "           0.0123,  1.1677,  0.1628]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "pos_emb = nn.Parameter(torch.randn(1, 12, 10))\n",
    "print(pos_emb)\n",
    "pos_emb[:, :2, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "\n",
    "    embedding_dropout = 0.1\n",
    "    residual_dropout = 0.1\n",
    "    attention_dropout = 0.1\n",
    "\n",
    "class GPT2Config(GPTConfig):\n",
    "    embedding_size = 768\n",
    "    n_heads = 12\n",
    "    n_layers = 12\n",
    "    max_sequence_length = 256\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPT2Config) -> None:\n",
    "        super().__init__()\n",
    "        assert config.embedding_size % config.n_heads == 0\n",
    "        self.values = nn.Linear(config.embedding_size, config.embedding_size)\n",
    "        self.keys = nn.Linear(config.embedding_size, config.embedding_size)\n",
    "        self.queries = nn.Linear(config.embedding_size, config.embedding_size)\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.residual_dropout = nn.Dropout(config.residual_dropout)\n",
    "\n",
    "        self.fc = nn.Linear(config.embedding_size, config.embedding_size)\n",
    "        \n",
    "        # triangular lower filled with ones \n",
    "        self.causal_mask = torch.tril(torch.ones(config.max_sequence_length, config.max_sequence_length)).view(1, 1, config.embedding_size, config.embedding_size)\n",
    "\n",
    "        self.n_heads = config.n_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, sequence_length, embed_size = x.shape\n",
    "\n",
    "        # B, sequence_length, n_heads, head_size\n",
    "        values = self.values(x).view(N, sequence_length, self.n_heads, embed_size // self.n_heads)\n",
    "        keys = self.keys(x).view(N, sequence_length, self.n_heads, embed_size // self.n_heads)\n",
    "        queries = self.queries(x).view(N, sequence_length, self.n_heads, embed_size // self.n_heads)\n",
    "\n",
    "        attention = torch.einsum('nqhd,nkhd->nhqk', [queries, keys]) * ( 1 / torch.sqrt(keys.shape[0]))\n",
    "        attention = attention.masked_fill(self.causal_mask == 0, float('-1e20'))\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        attention = self.attention_dropout(attention)\n",
    "        out = torch.einsum('nhqk,nkhd->nqhd', [attention, values]).reshape(N, sequence_length, embed_size)\n",
    "        out = self.residual_dropout(out)\n",
    "        return out\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPT2Config) -> None:\n",
    "        super().__init__()\n",
    "        self.norm_1 = nn.LayerNorm(config.embedding_size)\n",
    "        self.norm_2 = nn.LayerNorm(config.embedding_size)\n",
    "        self.attention = CausalSelfAttention(config)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(config.embedding_size, config.embedding_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.embedding_size * 4, config.embedding_size),\n",
    "            nn.Dropout(config.residual_dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm_1(x))\n",
    "        x = x + self.feed_forward(self.norm_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self, config: GPT2Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.word_embedding = nn.Embedding(config.vocab_size, config.embedding_size)\n",
    "        self.positional_embedding = nn.parameter(1, config.max_sequence_length, config.embedding_size)\n",
    "        self.dropout = nn.Dropout(config.embedding_dropout)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                Decoder(config) for _ in config.n_layers\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(config.embedding_size)\n",
    "        self.fc = nn.Linear(config.embedding_size, config.vocab_size, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, sequence_length = x.shape\n",
    "\n",
    "        # word_embedding.shape = batch_size x sequence_length x embedding_dim\n",
    "        word_embedding = self.word_embedding(x)\n",
    "        # positional_encoding.shape = 1 x sequence_length x embedding_dim\n",
    "        positional_encoding = self.positional_embedding[:, :sequence_length, :]\n",
    "        x = self.dropout(word_embedding + positional_encoding)\n",
    "        x = self.layers(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0444,   -inf,   -inf],\n",
       "         [0.0729, 0.5914,   -inf]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74f09c4dcce712dedd57a93287389ba50a94d19f06041f2f1e451d0a71cf11ad"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
