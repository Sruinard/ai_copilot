{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1639386118807
        }
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1639386129266
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_size, n_heads) -> None:\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dims = embed_size // n_heads\n",
        "\n",
        "        assert embed_size % n_heads == 0\n",
        "\n",
        "        self.values = nn.Linear(embed_size, embed_size)\n",
        "        self.keys = nn.Linear(embed_size, embed_size)\n",
        "        self.queries = nn.Linear(embed_size, embed_size)\n",
        "        self.fc = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "        \n",
        "    def forward(self, values, keys, queries, mask=None):\n",
        "        # values.shape = [batch_size, seq_len, embed_dim]\n",
        "        N = values.shape[0]\n",
        "        values_len = values.shape[1]\n",
        "        keys_len = keys.shape[1]\n",
        "        queries_len = queries.shape[1]\n",
        "\n",
        "        values = values.reshape(N, values_len, self.n_heads, self.head_dims)\n",
        "        keys = keys.reshape(N, keys_len, self.n_heads, self.head_dims)\n",
        "        queries = queries.reshape(N, queries_len, self.n_heads, self.head_dims)\n",
        "\n",
        "        x = self.scaled_dot_product(values, keys, queries, mask)\n",
        "        return self.fc(x)\n",
        "\n",
        "        \n",
        "\n",
        "    def scaled_dot_product(self, values, keys, queries, mask):\n",
        "        N = values.shape[0]\n",
        "        queries_len = queries.shape[1]\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "\n",
        "        if mask is not None:\n",
        "            energy.masked_fill(mask == 0, float('-1e20'))\n",
        "        \n",
        "        attention = F.softmax((energy / (self.embed_size ** 0.5)), dim=3)\n",
        "\n",
        "        out = torch.einsum('nhqk,nvhd->nqhd', [attention, values]).reshape(N, queries_len, self.n_heads * self.head_dims)\n",
        "        return out\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, n_heads, drop_out, forward_expension) -> None:\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.multi_head_attention = MultiHeadAttention(embed_size, n_heads)\n",
        "        self.norm_1 = nn.LayerNorm(embed_size)\n",
        "        self.norm_2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expension * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expension * embed_size, embed_size),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "\n",
        "    def forward(self, values, keys, queries, mask):\n",
        "        attenion = self.multi_head_attention(values, keys, queries, mask)\n",
        "        x = self.dropout(self.norm_1(attenion + queries))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm_2(forward + x))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, vocab_size, embed_size, n_layers, n_heads, forward_expansion, dropout, device, max_length\n",
        "        )  -> None:\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
        "        self.layers = nn.ModuleList([\n",
        "                TransformerBlock(embed_size, n_heads, drop_out=dropout, forward_expension=forward_expansion) \n",
        "            for _ in range(n_layers)\n",
        "        ]\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        N, sequence_length = x.shape\n",
        "        positions = torch.arange(0, sequence_length).expand(N, sequence_length).to(self.device)\n",
        "        x = self.dropout(self.word_embedding(x) + self.positional_embedding(positions))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, x, x, mask)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_size, n_heads, forward_expension, dropout, device) -> None:\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.attention = MultiHeadAttention(embed_size, n_heads)\n",
        "        self.norm = nn.LayerNorm(embed_size)\n",
        "        self.transformer_block = TransformerBlock(\n",
        "            embed_size, n_heads, dropout, forward_expension\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, values, keys, src_mask, trg_mask):\n",
        "        attention = self.attention(x, x, x, trg_mask)\n",
        "        queries = self.dropout(self.norm(attention + x))\n",
        "\n",
        "        out = self.transformer_block(values, keys, queries, src_mask)\n",
        "        return out\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, trg_vocab_size, embed_size, num_layers, n_heads, forward_expansion, dropout, device, max_length) -> None:\n",
        "        super(Decoder, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(trg_vocab_size, embed_size)\n",
        "        self.positional_embeddings = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(embed_size, n_heads, forward_expansion, dropout, device)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Linear(embed_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
        "        N, sequence_length = x.shape[0], x.shape[1]\n",
        "        positions = torch.arange(0, sequence_length).expand(N, sequence_length).to(device=self.device)\n",
        "        x = self.dropout(self.word_embeddings(x) + self.positional_embeddings(positions))\n",
        "        \n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
        "\n",
        "        out = self.fc(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, device='cuda', max_length=100) -> None:\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(src_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length)\n",
        "        self.decoder = Decoder(trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length)\n",
        "\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        pred = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
        "        return pred\n",
        "\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        N, trg_len = trg.shape\n",
        "        trg_mask = torch.tril(torch.ones(trg_len, trg_len)).expand(\n",
        "            N, 1, trg_len, trg_len\n",
        "        )\n",
        "        return trg_mask.to(self.device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "         3, 3, 3, 3, 3]])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "x = torch.tensor([[1, 2, 3, 4, 5, 6]]).to(device)\n",
        "y = torch.tensor([[4, 1, 3, 9, 5, 8]]).to(device)\n",
        "\n",
        "src_pad_idx = 0\n",
        "trg_pad_idx = 0\n",
        "src_vocab_size = 8\n",
        "trg_vocab_size = 10\n",
        "\n",
        "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, device=device).to(device)\n",
        "\n",
        "out = model(x, y[:, :1])\n",
        "probs, indx = torch.max(out, axis=-1)\n",
        "translation = torch.tensor([[1, 2]])\n",
        "for _ in range(99):\n",
        "    out = model(x, translation)\n",
        "    probs, indx = torch.max(out, axis=-1)\n",
        "    translation = torch.concat([translation[0], indx[0, -1:]]).unsqueeze(0)\n",
        "    # translation = torch.concat([translation[0], indx[0, -1:]]).unsqueeze(0)\n",
        "translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "filepath = \"../data/python/final/jsonl/train/python_train_0.jsonl\"\n",
        "with open(filepath) as f:\n",
        "    data = [json.loads(line) for line in f]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('def predict(X_img_path, knn_clf=None, model_path=None, '\n",
            " 'distance_threshold=0.6):\\n'\n",
            " '    \"\"\"\\n'\n",
            " '    Recognizes faces in given image using a trained KNN classifier\\n'\n",
            " '\\n'\n",
            " '    :param X_img_path: path to image to be recognized\\n'\n",
            " '    :param knn_clf: (optional) a knn classifier object. if not specified, '\n",
            " 'model_save_path must be specified.\\n'\n",
            " '    :param model_path: (optional) path to a pickled knn classifier. if not '\n",
            " 'specified, model_save_path must be knn_clf.\\n'\n",
            " '    :param distance_threshold: (optional) distance threshold for face '\n",
            " 'classification. the larger it is, the more chance\\n'\n",
            " '           of mis-classifying an unknown person as a known one.\\n'\n",
            " '    :return: a list of names and face locations for the recognized faces in '\n",
            " 'the image: [(name, bounding box), ...].\\n'\n",
            " \"        For faces of unrecognized persons, the name 'unknown' will be \"\n",
            " 'returned.\\n'\n",
            " '    \"\"\"\\n'\n",
            " '    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] '\n",
            " 'not in ALLOWED_EXTENSIONS:\\n'\n",
            " '        raise Exception(\"Invalid image path: {}\".format(X_img_path))\\n'\n",
            " '\\n'\n",
            " '    if knn_clf is None and model_path is None:\\n'\n",
            " '        raise Exception(\"Must supply knn classifier either thourgh knn_clf '\n",
            " 'or model_path\")\\n'\n",
            " '\\n'\n",
            " '    # Load a trained KNN model (if one was passed in)\\n'\n",
            " '    if knn_clf is None:\\n'\n",
            " \"        with open(model_path, 'rb') as f:\\n\"\n",
            " '            knn_clf = pickle.load(f)\\n'\n",
            " '\\n'\n",
            " '    # Load image file and find face locations\\n'\n",
            " '    X_img = face_recognition.load_image_file(X_img_path)\\n'\n",
            " '    X_face_locations = face_recognition.face_locations(X_img)\\n'\n",
            " '\\n'\n",
            " '    # If no faces are found in the image, return an empty result.\\n'\n",
            " '    if len(X_face_locations) == 0:\\n'\n",
            " '        return []\\n'\n",
            " '\\n'\n",
            " '    # Find encodings for faces in the test iamge\\n'\n",
            " '    faces_encodings = face_recognition.face_encodings(X_img, '\n",
            " 'known_face_locations=X_face_locations)\\n'\n",
            " '\\n'\n",
            " '    # Use the KNN model to find the best matches for the test face\\n'\n",
            " '    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)\\n'\n",
            " '    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in '\n",
            " 'range(len(X_face_locations))]\\n'\n",
            " '\\n'\n",
            " \"    # Predict classes and remove classifications that aren't within the \"\n",
            " 'threshold\\n'\n",
            " '    return [(pred, loc) if rec else (\"unknown\", loc) for pred, loc, rec in '\n",
            " 'zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]')\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "pprint.pprint(\n",
        "data[1][\"code\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage valid: 0.96\n"
          ]
        }
      ],
      "source": [
        "valid_samples = 0\n",
        "invalid_samples = 0\n",
        "for row in data:\n",
        "    func_length = len(row[\"code\"].split('\"\"\"'))\n",
        "    func_length_single_quote = len(row[\"code\"].split(\"'''\"))\n",
        "    if func_length == 3 or func_length_single_quote == 3:\n",
        "        valid_samples += 1\n",
        "        val_example = row\n",
        "    else:\n",
        "        invalid_samples +=1\n",
        "        inval_example = row\n",
        "print(f\"Percentage valid: {valid_samples / (valid_samples + invalid_samples)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[[ 0.7726,  0.1959, -0.2742, -0.4414, -0.1205,  0.5896,  0.2676,\n",
            "           0.2441,  1.3759, -0.0921],\n",
            "         [-0.7718, -1.2759, -0.1908,  0.4687,  0.5650,  0.2740,  0.0949,\n",
            "           0.0123,  1.1677,  0.1628],\n",
            "         [ 0.2920, -1.5287, -1.2610, -0.3102,  0.0399, -0.5319,  0.2206,\n",
            "          -0.8736,  0.2447, -0.4729],\n",
            "         [ 0.4001,  0.0745,  0.8375, -0.1703, -1.0116, -1.2133,  1.6221,\n",
            "          -0.9419, -1.3988,  0.0463],\n",
            "         [ 0.9865, -0.9952,  0.4861, -0.0090,  0.9086, -0.1690,  0.3133,\n",
            "           1.3860, -0.1374, -0.4883],\n",
            "         [ 1.0023, -1.8284,  0.3466,  1.0263, -1.5314, -1.4405,  1.4825,\n",
            "          -0.6440, -0.5110, -0.3031],\n",
            "         [-0.9094,  1.4731, -0.1728, -0.2754,  1.6843,  0.8862,  0.2473,\n",
            "           0.0916, -0.3818,  1.0779],\n",
            "         [ 0.6954, -0.9430, -0.3950,  0.9595, -1.8052,  0.6318,  1.0113,\n",
            "          -0.1440, -1.6109, -0.8765],\n",
            "         [-0.4137, -0.9999, -0.5800,  2.2038,  0.7193, -0.5238, -0.2172,\n",
            "           0.6618, -0.4866,  0.1218],\n",
            "         [-0.1828,  2.5232,  0.7855,  1.2489,  1.0359, -1.2540, -1.5563,\n",
            "          -1.0133,  0.7650, -0.3926],\n",
            "         [-1.8902, -0.8437,  2.4591,  0.3986,  2.0114,  0.5541,  1.2676,\n",
            "          -0.0705, -0.2490,  0.7999],\n",
            "         [ 0.0619, -1.0068,  0.9872,  0.1820,  1.8051, -0.6784, -0.1133,\n",
            "           0.4186, -0.0151, -0.3995]]], requires_grad=True)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[ 0.7726,  0.1959, -0.2742, -0.4414, -0.1205,  0.5896,  0.2676,\n",
              "           0.2441,  1.3759, -0.0921],\n",
              "         [-0.7718, -1.2759, -0.1908,  0.4687,  0.5650,  0.2740,  0.0949,\n",
              "           0.0123,  1.1677,  0.1628]]], grad_fn=<SliceBackward0>)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "pos_emb = nn.Parameter(torch.randn(1, 12, 10))\n",
        "print(pos_emb)\n",
        "pos_emb[:, :2, :]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPTConfig:\n",
        "\n",
        "    embedding_dropout = 0.1\n",
        "    residual_dropout = 0.1\n",
        "    attention_dropout = 0.1\n",
        "\n",
        "class GPT2Config(GPTConfig):\n",
        "    def __init__(self, vocab_size, embedding_size=768, n_heads=12, n_layers=12, max_sequence_length=256) -> None:\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config: GPT2Config) -> None:\n",
        "        super().__init__()\n",
        "        assert config.embedding_size % config.n_heads == 0\n",
        "        self.values = nn.Linear(config.embedding_size, config.embedding_size)\n",
        "        self.keys = nn.Linear(config.embedding_size, config.embedding_size)\n",
        "        self.queries = nn.Linear(config.embedding_size, config.embedding_size)\n",
        "\n",
        "        self.attention_dropout = nn.Dropout(config.attention_dropout)\n",
        "        self.residual_dropout = nn.Dropout(config.residual_dropout)\n",
        "\n",
        "        self.fc = nn.Linear(config.embedding_size, config.embedding_size)\n",
        "        \n",
        "        # triangular lower filled with ones \n",
        "        self.causal_mask = torch.tril(torch.ones(config.max_sequence_length, config.max_sequence_length)).view(1, 1, config.embedding_size, config.embedding_size)\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, sequence_length, embed_size = x.shape\n",
        "\n",
        "        # B, sequence_length, n_heads, head_size\n",
        "        values = self.values(x).view(N, sequence_length, self.n_heads, embed_size // self.n_heads)\n",
        "        keys = self.keys(x).view(N, sequence_length, self.n_heads, embed_size // self.n_heads)\n",
        "        queries = self.queries(x).view(N, sequence_length, self.n_heads, embed_size // self.n_heads)\n",
        "\n",
        "        attention = torch.einsum('nqhd,nkhd->nhqk', [queries, keys]) * ( 1 / torch.sqrt(keys.shape[0]))\n",
        "        attention = attention.masked_fill(self.causal_mask == 0, float('-1e20'))\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attention_dropout(attention)\n",
        "        out = torch.einsum('nhqk,nkhd->nqhd', [attention, values]).reshape(N, sequence_length, embed_size)\n",
        "        out = self.residual_dropout(out)\n",
        "        return out\n",
        "        \n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, config: GPT2Config) -> None:\n",
        "        super().__init__()\n",
        "        self.norm_1 = nn.LayerNorm(config.embedding_size)\n",
        "        self.norm_2 = nn.LayerNorm(config.embedding_size)\n",
        "        self.attention = CausalSelfAttention(config)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(config.embedding_size, config.embedding_size * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(config.embedding_size * 4, config.embedding_size),\n",
        "            nn.Dropout(config.residual_dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(self.norm_1(x))\n",
        "        x = x + self.feed_forward(self.norm_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
        "\n",
        "    def __init__(self, config: GPT2Config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.word_embedding = nn.Embedding(config.vocab_size, config.embedding_size)\n",
        "        self.positional_embedding = nn.parameter(1, config.max_sequence_length, config.embedding_size)\n",
        "        self.dropout = nn.Dropout(config.embedding_dropout)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                Decoder(config) for _ in config.n_layers\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(config.embedding_size)\n",
        "        self.fc = nn.Linear(config.embedding_size, config.vocab_size, bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, sequence_length = x.shape\n",
        "\n",
        "        # word_embedding.shape = batch_size x sequence_length x embedding_dim\n",
        "        word_embedding = self.word_embedding(x)\n",
        "        # positional_encoding.shape = 1 x sequence_length x embedding_dim\n",
        "        positional_encoding = self.positional_embedding[:, :sequence_length, :]\n",
        "        x = self.dropout(word_embedding + positional_encoding)\n",
        "        x = self.layers(x)\n",
        "        x = self.layer_norm(x)\n",
        "        logits = self.fc(x)\n",
        "        return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "value = 10 if True else 8\n",
        "value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
            "ERROR: could not open HSTS store at '/home/sruinard/.wget-hsts'. HSTS will be disabled.\n",
            "--2021-12-11 18:26:16--  https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt               [ <=>                ] 159.34K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-12-11 18:26:16 (2.91 MB/s) - ‘input.txt’ saved [163162]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "        \n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        \"\"\"\n",
        "        arrange data and targets so that the first i elements of x\n",
        "        will be asked to predict the i-th element of y. Notice that\n",
        "        the eventual language model will actually make block_size\n",
        "        individual predictions at the same time based on this data,\n",
        "        so we are being clever and amortizing the cost of the forward\n",
        "        pass of the network. So for example if block_size is 4, then\n",
        "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
        "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
        "        then actually \"multitask\" 4 separate examples at the same time\n",
        "        in the language model:\n",
        "        - given just \"h\", please predict \"e\" as next\n",
        "        - given \"he\" please predict \"l\" next\n",
        "        - given \"hel\" predict \"l\" next\n",
        "        - given \"hell\" predict \"o\" next\n",
        "        \n",
        "        In addition, because the DataLoader will create batches of examples,\n",
        "        every forward/backward pass during traning will simultaneously train\n",
        "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
        "        for a batched input of integers X (B, T) where B is batch size and\n",
        "        T is block_size and Y (B, T), the network will during training be\n",
        "        simultaneously training to make B*T predictions, all at once! Of course,\n",
        "        at test time we can paralellize across batch B, but unlike during training\n",
        "        we cannot parallelize across the time dimension T - we have to run\n",
        "        a forward pass of the network to recover the next single character of the \n",
        "        sequence along each batch dimension, and repeatedly always feed in a next\n",
        "        character to get the next one.\n",
        "        \n",
        "        So yes there is a big asymmetry between train/test time of autoregressive\n",
        "        models. During training we can go B*T at a time with every forward pass,\n",
        "        but during test time we can only go B at a time, T times, with T forward \n",
        "        passes.\n",
        "        \"\"\"\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1639386134465
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "os.getcwd()\n",
        "\n",
        "sys.path.append(\"../\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data has 163134 characters, 95 unique.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 1 iter 35: train loss 2.33950. lr 5.999840e-04:   1%|          | 36/5094 [02:23<5:41:20,  4.05s/it]"
          ]
        }
      ],
      "source": [
        "from trainer.model import GPT, GPT2Config\n",
        "max_sequence_length = 128 # spatial extent of the model for its context\n",
        "data = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
        "train_dataset = CharDataset(data, max_sequence_length)\n",
        "\n",
        "mconf = GPT2Config(\n",
        "    vocab_size=train_dataset.vocab_size, \n",
        "    max_sequence_length=max_sequence_length,\n",
        "    n_layers=3,\n",
        "    n_heads=4\n",
        ")\n",
        "\n",
        "# mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
        "#                   n_layer=3, n_head=4, n_embd=512)\n",
        "model = GPT(mconf)\n",
        "\n",
        "from trainer.task import Trainer, TrainerConfig\n",
        "\n",
        "# initialize a trainer instance and kick off training\n",
        "tconf = TrainerConfig(max_epochs=2, batch_size=32, learning_rate=6e-4,\n",
        "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*max_sequence_length,\n",
        "                      num_workers=4)\n",
        "trainer = Trainer(model, train_dataset, None, tconf)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "256.0"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../data'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9b3994b915f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data'"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 1  2  3  4  5 -1  1  2  3  4  5]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "X = np.arange(100).reshape(-1, 10)\n",
        "\n",
        "\n",
        "class CodeToTextDataset():\n",
        "    def __init__(self, data, sequence_length):\n",
        "        self.data = data\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sequence_length = 4\n",
        "\n",
        "test_batch = np.array([\n",
        "    [1, 2, 3, 4, 5, -1, 1, 2, 3, 4, 5],\n",
        "    [10, 11, 12, 13, -1, 5, 6, 7, 8, 9, 0],\n",
        "])\n",
        "\n",
        "ds = CodeToTextDataset(data=test_batch, sequence_length=sequence_length)\n",
        "SOS = -1\n",
        "\n",
        "# [1, 2, 3, 4, 5, -1, 1, 2, 3, 4, 5],\n",
        "# desired output:\n",
        "# [1, 2, 3, 4] [5]\n",
        "# [2, 3, 4, 5] [-1]\n",
        "# [3, 4, 5, -1] [1]\n",
        "import numpy as np\n",
        "def test_sample_ds(ds=ds):\n",
        "    sample = next(iter(ds))\n",
        "    split_index = np.argmax(sample == SOS)\n",
        "    assert split_index == 5\n",
        "\n",
        "    max_sample_index = 5\n",
        "    \n",
        "\n",
        "test_sample_ds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torch\n",
        "from torchtext.datasets import AG_NEWS\n",
        "train_iter = AG_NEWS(split='train')\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter = AG_NEWS(split='train')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
        "\n",
        "train_iter = AG_NEWS(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([0, 1, 2, 3]),\n",
              " tensor([1, 2, 3, 4]),\n",
              " tensor([2, 3, 4, 5]),\n",
              " tensor([3, 4, 5, 6]),\n",
              " tensor([4, 5, 6, 7]),\n",
              " tensor([5, 6, 7, 8])]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequence_length = 6\n",
        "sample = torch.arange(10)\n",
        "\n",
        "[sample[i: i+sequence_length] for i in range(len(sample) - sequence_length)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([  860,     4,     2,  2242,   725,     5,  4788,  1408,    80,    62,\n",
              "            4,  3292,   165,   122,   746,     5,   413, 18272,    11,  1013,\n",
              "        13356,   798])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hello this is sentence 1 just another sentence'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame([\"hello this is sentence 1\", \"just another sentence\"], columns=['text'])['text'].str.cat(sep=' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement transformers-tokenizers (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for transformers-tokenizers\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers-tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 1.8 MB/s eta 0:00:01\n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.10.3\n"
          ]
        }
      ],
      "source": [
        "%pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[SOS]\", \"[EOS]\", \"[PAD]\", \"[MASK]\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace, ByteLevel\n",
        "from tokenizers import Tokenizer, ByteLevelBPETokenizer\n",
        "from tokenizers.models import BPE\n",
        "\n",
        "tokenizer = Tokenizer(BPE())\n",
        "tokenizer.pre_tokenizer = ByteLevel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "files = ['./vocab_file.txt']\n",
        "tokenizer.train(files, trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "vocab_file_dataset_dest = \"./vocab_dataset_file.txt\" \n",
        "filepaths = [\"../data/python/final/jsonl/train/python_train_0.jsonl\", \"../data/python/final/jsonl/train/python_train_0.jsonl\"]\n",
        "\n",
        "def build_vocab_dataset(filepaths: List[str], vocab_file_dataset_dest: str) -> None:\n",
        "    if os.path.exists(vocab_file_dataset_dest):\n",
        "        os.remove(vocab_file_dataset_dest)\n",
        "    for filepath in filepaths:\n",
        "        with open(filepath, 'r') as f:\n",
        "            lines = [json.loads(line) for line in f]\n",
        "        with open(vocab_file_dataset_dest, \"w+\") as f:\n",
        "            for line in lines:\n",
        "                f.write(line['code'])\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "class Dataset:\n",
        "\n",
        "    def __init__(self, raw_files: List[str], dataset_dest: str):\n",
        "        self.raw_files = raw_files\n",
        "        self.dataset_dest = dataset_dest\n",
        "\n",
        "    def build_dataset(self):\n",
        "        if os.path.exists(self.dataset_dest):\n",
        "            os.remove(self.dataset_dest)\n",
        "        for filepath in filepaths:\n",
        "            with gzip.open(filepath, 'r') as f:\n",
        "                lines = [json.loads(line) for line in f]\n",
        "            with open(vocab_file_dataset_dest, \"w+\") as f:\n",
        "                for line in lines:\n",
        "                    f.write(line['code'])\n",
        "                    f.write(\"\\n\")\n",
        "\n",
        "class Tokenizer:\n",
        "\n",
        "    def __init__(self, tokenizer, special_tokens):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.special_tokens = special_tokens\n",
        "\n",
        "    def save_tokenizer(self, filepath):\n",
        "        self.tokenizer.save(filepath)\n",
        "    \n",
        "    def load_tokenizer(self, filepath):\n",
        "        self.tokenizer.load(filepath)\n",
        "\n",
        "    def fit(self, files, path_to_save_tokenizer:str = None):\n",
        "        self.tokenizer.train(\n",
        "            files=files,\n",
        "            min_frequency=2,\n",
        "            special_tokens=self.special_tokens\n",
        "        )\n",
        "        if path_to_save_tokenizer is not None:\n",
        "            self.save_tokenizer(path_to_save_tokenizer)\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = \"/home/sruinard/documents/personal/ai_copilot/data/python/final/jsonl/train/python_train_1.jsonl.gz\"\n",
        "import gzip\n",
        "import json\n",
        "with gzip.open(path, 'r') as f:\n",
        "    lines = [json.loads(line) for line in f]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample = \"def hello_world(name): \\n\\t value = 10\" + \" <eos>\"\n",
        "out = tokenizer.encode(sample)\n",
        "print(out.tokens)\n",
        "tokenizer.decode(out.ids)\n",
        "\n",
        "with open('./tmp_vocab.txt', 'w') as f:\n",
        "    f.write(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "['def', 'Ġhello', '_', 'world', '(', 'name', '):', 'ĠĊ', 'ĉ', 'Ġvalue', 'Ġ=', 'Ġ10', 'Ġ', '<eos>']\n",
            "def hello_world(name): \n",
            "\t value = 10 \n"
          ]
        }
      ],
      "source": [
        "\n",
        " # Initialize tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Train tokenizer\n",
        "tokenizer.train(files=['./vocab_file.txt'],\n",
        "                    # vocab_size=32000,\n",
        "                    min_frequency=2,\n",
        "                    special_tokens=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"],\n",
        "                    )\n",
        "\n",
        "# Save vocabulary and merges\n",
        "e = tokenizer.encode(sample)\n",
        "d = tokenizer.decode(e.ids)\n",
        "print(e.tokens)\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.save(\"mytokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'def hello_world(name): \\n\\t value = 10 <eos>'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# new_tokenizer = ByteLevelBPETokenizer()\n",
        "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
        "new_tokenizer = PreTrainedTokenizerFast(tokenizer_file='./mytokenizer')\n",
        "# assert new_tokenizer.encode(sample).tokens == []\n",
        "new_tokenizer.decode(new_tokenizer.encode(sample))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"test.txt\", \"w\") as f:\n",
        "    f.write(\"def hello(name): \\n    print(f'hello {name}'\")\n",
        "    f.write(\" [eos] \\n\")\n",
        "    f.write(\"def second_hello(name): \\n    print(f'hello {name}' <eos>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"test.txt\", 'r') as f:\n",
        "    data = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"./vocab_file.txt\") as f:\n",
        "    data = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_data = tokenizer.encode(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './transform_dir/transform_samples/tokenized_data'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_27276/2666986291.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./transform_dir/transform_samples/tokenized_data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# os.removedirs(\"./transform_dir/transform_samples\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./transform_dir/transform_samples/tokenized_data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './transform_dir/transform_samples/tokenized_data'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "with open(\"./transform_dir/transform_samples/tokenized_data\", 'w') as f:\n",
        "    f.write(json.dumps(tokenized_data[:20]))\n",
        "# os.removedirs(\"./transform_dir/transform_samples\")\n",
        "with open(\"./transform_dir/transform_samples/tokenized_data\", \"r\") as f:\n",
        "    loaded_tokens = f.read()\n",
        "\n",
        "json.loads(loaded_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[344,\n",
              " 21674,\n",
              " 66,\n",
              " 8645,\n",
              " 890,\n",
              " 367,\n",
              " 4633,\n",
              " 557,\n",
              " 11310,\n",
              " 21674,\n",
              " 10072,\n",
              " 356,\n",
              " 2,\n",
              " 4184,\n",
              " 1155,\n",
              " 413,\n",
              " 16793,\n",
              " 10072,\n",
              " 471,\n",
              " 3,\n",
              " 202]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_sample = 'def hello_world(): \"\"\"this def prints hello world\"\"\"<bos> \\n print(\\'hello world\\')<eos>\\n'\n",
        "tokenized_data = new_tokenizer.encode(test_sample)\n",
        "tokenized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9890937"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenized_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "save_path: (optional) path to save model on disk\n",
            "    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n",
            "    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n",
            "    :param verbose: verbosity of training\n",
            "    :return: returns knn classifier that was trained on the given data.\n",
            "    \"\"\"\n",
            "    X = []\n",
            "    y = []\n",
            "\n",
            "    # Loop through each person in the training set\n",
            "    for class_dir in os.listdir(train_dir):\n",
            "        if not os.path.isdir(os.path.join(train_dir, class_dir)):\n",
            "            continue\n",
            "\n",
            "        # Loop through each training image for the current person\n",
            "        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n",
            "            image = face_recognition.load_image_file(img_path)\n",
            "            face_bounding_boxes = face_recognition.face_locations(image)\n",
            "\n",
            "            if len(face_bounding_boxes) != 1:\n",
            "                # If there are no people (\n"
          ]
        }
      ],
      "source": [
        "sequence_length = 256\n",
        "idx = 200\n",
        "print(tokenizer.decode(tokenized_data.ids[idx: idx+sequence_length]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1639386160503
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "from utils.build_dataset import Dataset\n",
        "run_id = 1\n",
        "pipeline_dir = os.path.join(\"pipeline_runs\", str(run_id))\n",
        "example_gen = os.path.join(pipeline_dir, 'example_gen')\n",
        "[os.makedirs(dir, exist_ok=True) for dir in [pipeline_dir, example_gen]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "raw_files = [\"../data/python/final/jsonl/train/python_train_1.jsonl.gz\"]\n",
        "dataset_dest = os.path.join(example_gen, \"code_dataset.txt\")\n",
        "code_dataset = Dataset(raw_files, dataset_dest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "code_dataset.build_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'pipeline_runs/1/transform'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transform_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trainer.preprocessing import Tokenizer\n",
        "from trainer.constants import SpecialTokens\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "\n",
        "special_tokens = [SpecialTokens.BOS, SpecialTokens.EOS]\n",
        "tokenizer_instance = ByteLevelBPETokenizer()\n",
        "transform_dir = os.path.join(pipeline_dir, \"transform\")\n",
        "tokenizer_filename = 'tokenizer'\n",
        "preprocessed_features_filename = \"preprocessed.txt\"\n",
        "tokenizer = Tokenizer(tokenizer=tokenizer_instance, special_tokens=special_tokens, transform_output_dir=transform_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer.fit([dataset_dest], tokenizer_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.transform([dataset_dest], processed_data_filename=preprocessed_features_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"../experiments/pipeline_runs/1/transform/transform_fn/tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trainer.task import TextToCodeDataset, Trainer, TrainerConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "sequence_length = 128\n",
        "train_dataset = TextToCodeDataset(transform_dir, sequence_length)\n",
        "\n",
        "from trainer.model import GPT, GPT2Config\n",
        "\n",
        "mconf = GPT2Config(\n",
        "    new_tokenizer.vocab_size,\n",
        "    embedding_size=768,\n",
        "    n_heads=12,\n",
        "    n_layers=3,\n",
        "    max_sequence_length=sequence_length\n",
        ")\n",
        "\n",
        "\n",
        "model = GPT(mconf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "tconf = TrainerConfig(\n",
        "    max_epochs=2, \n",
        "    batch_size=32, \n",
        "    learning_rate=6e-4,\n",
        "    lr_decay=True, \n",
        "    warmup_tokens=512*20,\n",
        "    final_tokens=2*len(train_dataset)*sequence_length,\n",
        "    num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(model, train_dataset, None, tconf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 1 iter 6: train loss 7.03550. lr 6.000000e-04:   0%|          | 7/278140 [01:09<799:23:38, 10.35s/it] "
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace, Dataset\n",
        "from azureml.core.environment import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "ws = Workspace.from_config()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['azureml-defaults', 'transformers==4.5.1', 'tokenizers==0.10.3']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{\n",
              "    \"databricks\": {\n",
              "        \"eggLibraries\": [],\n",
              "        \"jarLibraries\": [],\n",
              "        \"mavenLibraries\": [],\n",
              "        \"pypiLibraries\": [],\n",
              "        \"rcranLibraries\": []\n",
              "    },\n",
              "    \"docker\": {\n",
              "        \"arguments\": [],\n",
              "        \"baseDockerfile\": \"FROM mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20211124.v1\\n\\nENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/pytorch-1.10\\n\\n# Create conda environment\\nRUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\\\\n    python=3.8 \\\\\\n    pip=20.2.4 \\\\\\n    pytorch=1.10.0 \\\\\\n    torchvision=0.11.1 \\\\\\n    torchaudio=0.10.0 \\\\\\n    cudatoolkit=11.1.1 \\\\\\n    nvidia-apex=0.1.0 \\\\\\n    gxx_linux-64 \\\\\\n    -c anaconda -c pytorch -c conda-forge\\n\\n# Prepend path to AzureML conda environment\\nENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH/bin:$PATH\\n\\n# Install pip dependencies\\nRUN pip install 'matplotlib>=3.3,<3.4' \\\\\\n                'psutil>=5.8,<5.9' \\\\\\n                'tqdm>=4.59,<4.63' \\\\\\n                'pandas>=1.3,<1.4' \\\\\\n                'scipy>=1.5,<1.8' \\\\\\n                'numpy>=1.10,<1.22' \\\\\\n                'ipykernel~=6.0' \\\\\\n                'azureml-core==1.36.0.post2' \\\\\\n                'azureml-defaults==1.36.0' \\\\\\n                'azureml-mlflow==1.36.0' \\\\\\n                'azureml-telemetry==1.36.0' \\\\\\n                'tensorboard==2.6.0' \\\\\\n                'tensorflow-gpu==2.6.0' \\\\\\n                'onnxruntime-gpu>=1.7,<1.10' \\\\\\n                'horovod==0.23' \\\\\\n                'future==0.18.2' \\\\\\n                'torch-tb-profiler==0.3.1'\\n\\n\\n# This is needed for mpi to locate libpython\\nENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH/lib:$LD_LIBRARY_PATH\\n\",\n",
              "        \"baseImage\": null,\n",
              "        \"baseImageRegistry\": {\n",
              "            \"address\": null,\n",
              "            \"password\": null,\n",
              "            \"registryIdentity\": null,\n",
              "            \"username\": null\n",
              "        },\n",
              "        \"enabled\": false,\n",
              "        \"platform\": {\n",
              "            \"architecture\": \"amd64\",\n",
              "            \"os\": \"Linux\"\n",
              "        },\n",
              "        \"sharedVolumes\": true,\n",
              "        \"shmSize\": null\n",
              "    },\n",
              "    \"environmentVariables\": {\n",
              "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
              "    },\n",
              "    \"inferencingStackVersion\": null,\n",
              "    \"name\": \"aicopilot_curated\",\n",
              "    \"python\": {\n",
              "        \"baseCondaEnvironment\": null,\n",
              "        \"condaDependencies\": {\n",
              "            \"channels\": [\n",
              "                \"anaconda\",\n",
              "                \"conda-forge\"\n",
              "            ],\n",
              "            \"dependencies\": [\n",
              "                \"python=3.6.2\",\n",
              "                {\n",
              "                    \"pip\": [\n",
              "                        \"azureml-defaults\",\n",
              "                        \"transformers==4.5.1\",\n",
              "                        \"tokenizers==0.10.3\"\n",
              "                    ]\n",
              "                }\n",
              "            ],\n",
              "            \"name\": \"project_environment\"\n",
              "        },\n",
              "        \"condaDependenciesFile\": null,\n",
              "        \"interpreterPath\": \"python\",\n",
              "        \"userManagedDependencies\": true\n",
              "    },\n",
              "    \"r\": null,\n",
              "    \"spark\": {\n",
              "        \"packages\": [],\n",
              "        \"precachePackages\": true,\n",
              "        \"repositories\": []\n",
              "    },\n",
              "    \"version\": \"1\"\n",
              "}"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "env = Environment.get(ws, \"AzureML-pytorch-1.10-ubuntu18.04-py38-cuda11-gpu\")\n",
        "environment = env.clone(\"aicopilot_curated\")\n",
        "conda_dep = CondaDependencies()\n",
        "conda_dep.add_pip_package(\"transformers==4.5.1\")\n",
        "conda_dep.add_pip_package(\"tokenizers==0.10.3\")\n",
        "print(list(conda_dep.pip_packages))\n",
        "environment.python.conda_dependencies=conda_dep\n",
        "environment.register(ws)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              "    \"databricks\": {\n",
              "        \"eggLibraries\": [],\n",
              "        \"jarLibraries\": [],\n",
              "        \"mavenLibraries\": [],\n",
              "        \"pypiLibraries\": [],\n",
              "        \"rcranLibraries\": []\n",
              "    },\n",
              "    \"docker\": {\n",
              "        \"arguments\": [],\n",
              "        \"baseDockerfile\": null,\n",
              "        \"baseImage\": \"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210806.v1\",\n",
              "        \"baseImageRegistry\": {\n",
              "            \"address\": null,\n",
              "            \"password\": null,\n",
              "            \"registryIdentity\": null,\n",
              "            \"username\": null\n",
              "        },\n",
              "        \"enabled\": false,\n",
              "        \"platform\": {\n",
              "            \"architecture\": \"amd64\",\n",
              "            \"os\": \"Linux\"\n",
              "        },\n",
              "        \"sharedVolumes\": true,\n",
              "        \"shmSize\": null\n",
              "    },\n",
              "    \"environmentVariables\": {\n",
              "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
              "    },\n",
              "    \"inferencingStackVersion\": null,\n",
              "    \"name\": \"aicopilot\",\n",
              "    \"python\": {\n",
              "        \"baseCondaEnvironment\": null,\n",
              "        \"condaDependencies\": {\n",
              "            \"channels\": [\n",
              "                \"anaconda\",\n",
              "                \"conda-forge\"\n",
              "            ],\n",
              "            \"dependencies\": [\n",
              "                \"python=3.6.2\",\n",
              "                {\n",
              "                    \"pip\": [\n",
              "                        \"torch==1.9.0\",\n",
              "                        \"tokenizers==0.10.3\",\n",
              "                        \"transformers==4.5.1\",\n",
              "                        \"numpy==1.18.5\"\n",
              "                    ]\n",
              "                },\n",
              "                \"pip\"\n",
              "            ],\n",
              "            \"name\": \"azureml_2f3f59620bf0165bc1a6f3286e72148d\"\n",
              "        },\n",
              "        \"condaDependenciesFile\": null,\n",
              "        \"interpreterPath\": \"python\",\n",
              "        \"userManagedDependencies\": false\n",
              "    },\n",
              "    \"r\": null,\n",
              "    \"spark\": {\n",
              "        \"packages\": [],\n",
              "        \"precachePackages\": true,\n",
              "        \"repositories\": []\n",
              "    },\n",
              "    \"version\": \"2\"\n",
              "}"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "environment = Environment.from_pip_requirements('aicopilot', file_path='./requirements.txt')\n",
        "# conda_dep = CondaDependencies()\n",
        "# conda_dep.add_pip_package(\"transformers==4.5.1\")\n",
        "# conda_dep.add_pip_package(\"tokenizers==0.10.3\")\n",
        "# environment.python.conda_dependencies=conda_dep\n",
        "environment.register(ws)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              "    \"databricks\": {\n",
              "        \"eggLibraries\": [],\n",
              "        \"jarLibraries\": [],\n",
              "        \"mavenLibraries\": [],\n",
              "        \"pypiLibraries\": [],\n",
              "        \"rcranLibraries\": []\n",
              "    },\n",
              "    \"docker\": {\n",
              "        \"arguments\": [],\n",
              "        \"baseDockerfile\": null,\n",
              "        \"baseImage\": \"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210806.v1\",\n",
              "        \"baseImageRegistry\": {\n",
              "            \"address\": null,\n",
              "            \"password\": null,\n",
              "            \"registryIdentity\": null,\n",
              "            \"username\": null\n",
              "        },\n",
              "        \"enabled\": false,\n",
              "        \"platform\": {\n",
              "            \"architecture\": \"amd64\",\n",
              "            \"os\": \"Linux\"\n",
              "        },\n",
              "        \"sharedVolumes\": true,\n",
              "        \"shmSize\": null\n",
              "    },\n",
              "    \"environmentVariables\": {\n",
              "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
              "    },\n",
              "    \"inferencingStackVersion\": null,\n",
              "    \"name\": \"aicopilot_from_req\",\n",
              "    \"python\": {\n",
              "        \"baseCondaEnvironment\": null,\n",
              "        \"condaDependencies\": {\n",
              "            \"channels\": [\n",
              "                \"anaconda\",\n",
              "                \"conda-forge\"\n",
              "            ],\n",
              "            \"dependencies\": [\n",
              "                \"python=3.6.2\",\n",
              "                {\n",
              "                    \"pip\": [\n",
              "                        \"tokenizers\",\n",
              "                        \"huggingface-transformers\"\n",
              "                    ]\n",
              "                },\n",
              "                \"pip\"\n",
              "            ],\n",
              "            \"name\": \"azureml_fa03d36e09ab2c57b3111def600b8d84\"\n",
              "        },\n",
              "        \"condaDependenciesFile\": null,\n",
              "        \"interpreterPath\": \"python\",\n",
              "        \"userManagedDependencies\": false\n",
              "    },\n",
              "    \"r\": null,\n",
              "    \"spark\": {\n",
              "        \"packages\": [],\n",
              "        \"precachePackages\": true,\n",
              "        \"repositories\": []\n",
              "    },\n",
              "    \"version\": \"2\"\n",
              "}"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# env = Environment.get(ws, \"AzureML-pytorch-1.10-ubuntu18.04-py38-cuda11-gpu\")\n",
        "curated_env = Environment.from_pip_requirements('aicopilot_from_req', file_path='./requirements.txt')\n",
        "curated_env.register(ws)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "loaded_env = Environment.get(ws, 'aicopilot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'imageExistsInRegistry': True, 'intellectualPropertyPublisher': None, 'imageCapabilities': {'canAccessData': True}, 'ingredients': {'dockerfile': 'FROM mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20211124.v1\\n\\nENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/pytorch-1.10\\n\\n# Create conda environment\\nRUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\\\\n    python=3.8 \\\\\\n    pip=20.2.4 \\\\\\n    pytorch=1.10.0 \\\\\\n    torchvision=0.11.1 \\\\\\n    torchaudio=0.10.0 \\\\\\n    cudatoolkit=11.1.1 \\\\\\n    nvidia-apex=0.1.0 \\\\\\n    gxx_linux-64 \\\\\\n    -c anaconda -c pytorch -c conda-forge\\n\\n# Prepend path to AzureML conda environment\\nENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH/bin:$PATH\\n\\n# Install pip dependencies\\nRUN pip install \\'matplotlib>=3.3,<3.4\\' \\\\\\n                \\'psutil>=5.8,<5.9\\' \\\\\\n                \\'tqdm>=4.59,<4.63\\' \\\\\\n                \\'pandas>=1.3,<1.4\\' \\\\\\n                \\'scipy>=1.5,<1.8\\' \\\\\\n                \\'numpy>=1.10,<1.22\\' \\\\\\n                \\'ipykernel~=6.0\\' \\\\\\n                \\'azureml-core==1.36.0.post2\\' \\\\\\n                \\'azureml-defaults==1.36.0\\' \\\\\\n                \\'azureml-mlflow==1.36.0\\' \\\\\\n                \\'azureml-telemetry==1.36.0\\' \\\\\\n                \\'tensorboard==2.6.0\\' \\\\\\n                \\'tensorflow-gpu==2.6.0\\' \\\\\\n                \\'onnxruntime-gpu>=1.7,<1.10\\' \\\\\\n                \\'horovod==0.23\\' \\\\\\n                \\'future==0.18.2\\' \\\\\\n                \\'torch-tb-profiler==0.3.1\\'\\n\\n\\n# This is needed for mpi to locate libpython\\nENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH/lib:$LD_LIBRARY_PATH\\n\\nUSER root\\nRUN mkdir -p $HOME/.cache\\nWORKDIR /\\nCOPY azureml-environment-setup/99brokenproxy /etc/apt/apt.conf.d/\\nCOPY azureml-environment-setup/spark_cache.py azureml-environment-setup/log4j.properties /azureml-environment-setup/\\nRUN if [ $SPARK_HOME ]; then /bin/bash -c \\'$SPARK_HOME/bin/spark-submit  /azureml-environment-setup/spark_cache.py\\'; fi\\nENV AZUREML_ENVIRONMENT_IMAGE True\\nCMD [\"bash\"]\\n', 'condaSpecification': None}, 'pythonEnvironment': {'interpreterPath': 'python', 'condaEnvironmentName': None, 'condaEnvironmentPath': None}, 'dockerImage': {'name': 'azureml/azureml_8d269b1c72ca1a778e11b1f5b7ea5da6', 'registry': {'address': 'viennaglobal.azurecr.io', 'username': 'b7b***', 'password': '***'}}, 'warnings': []}"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loaded_env.get_image_details(ws)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = Dataset.get_by_name(ws, 'code_to_text_python')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/data/test/python_test_0.jsonl.gz',\n",
              " '/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/data/train/python_train_0.jsonl.gz',\n",
              " '/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/data/train/python_train_1.jsonl.gz',\n",
              " '/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/data/train/python_train_10.jsonl.gz',\n",
              " '/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/data/train/python_train_11.jsonl.gz',\n",
              " '/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/data/train/python_train_12.jsonl.gz',\n",
              " '/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/data/train/python_train_13.jsonl.gz',\n",
              " '/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/data/train/python_train_2.jsonl.gz',\n",
              " '/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/data/train/python_train_3.jsonl.gz',\n",
              " '/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/data/train/python_train_4.jsonl.gz',\n",
              " '/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/data/train/python_train_5.jsonl.gz',\n",
              " '/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/data/train/python_train_6.jsonl.gz',\n",
              " '/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/data/train/python_train_7.jsonl.gz',\n",
              " '/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/data/train/python_train_8.jsonl.gz',\n",
              " '/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/data/train/python_train_9.jsonl.gz',\n",
              " '/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/data/valid/python_valid_0.jsonl.gz']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.makedirs(\"../data\", exist_ok=True)\n",
        "dataset.download(\"../data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-9def5255f902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmount_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/azureml/data/_loggerfactory.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_LoggerFactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_activity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivity_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_dimensions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'activity_info'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'error_code'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/azureml/data/file_dataset.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(self, mount_point, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0minvocation_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muuid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         dataflow = get_dataflow_for_execution(self._dataflow, 'mount.find_prefix', 'FileDataset',\n\u001b[0m\u001b[1;32m    226\u001b[0m                                               invocation_id=invocation_id)\n\u001b[1;32m    227\u001b[0m         \u001b[0mbase_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_path_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/azureml/data/_loggerfactory.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_LoggerFactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_activity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivity_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_dimensions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'activity_info'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'error_code'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/azureml/data/abstract_dataset.py\u001b[0m in \u001b[0;36m_dataflow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUserErrorException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset definition is missing. Please check how the dataset is created.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registration\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mdataprep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datastore_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_auth_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_definition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataprep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/azureml/dataprep/api/_datastore_helper.py\u001b[0m in \u001b[0;36m_set_auth_type\u001b[0;34m(workspace)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mauth_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mget_engine_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_aml_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSetAmlAuthMessageArgument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/azureml/dataprep/api/_aml_helper.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(op_code, message, cancellation_token)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchanged\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mengine_api_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_environment_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchanged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msend_message_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/azureml/dataprep/api/engineapi/api.py\u001b[0m in \u001b[0;36mset_aml_auth\u001b[0;34m(self, message_args, cancellation_token)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mupdate_aml_env_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_engine_api\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_aml_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtypedefinitions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetAmlAuthMessageArgument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCancellationToken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_channel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Engine.SetAmlAuth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/azureml/dataprep/api/engineapi/engine.py\u001b[0m in \u001b[0;36msend_message\u001b[0;34m(self, op_code, message, cancellation_token)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_relaunch_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_was_relaunched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_relaunch_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_message_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/azureml/dataprep/api/engineapi/api.py\u001b[0m in \u001b[0;36mconnect_to_requests_channel\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconnect_to_requests_channel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine_server_secret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_host_secret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequests_channel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost_secret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine_server_port\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_host_channel_port\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequests_channel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/azureml/dataprep/api/_aml_helper.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(op_code, message, cancellation_token)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchanged\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mengine_api_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_environment_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchanged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msend_message_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/azureml/dataprep/api/engineapi/api.py\u001b[0m in \u001b[0;36msync_host_secret\u001b[0;34m(self, message_args, cancellation_token)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mupdate_aml_env_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_engine_api\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msync_host_secret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCancellationToken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_channel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Engine.SyncHostSecret'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/site-packages/azureml/dataprep/api/engineapi/engine.py\u001b[0m in \u001b[0;36msend_message\u001b[0;34m(self, op_code, message, cancellation_token)\u001b[0m\n\u001b[1;32m    269\u001b[0m         }, cls=CustomEncoder))\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_messages_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pending_messages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38_pytorch/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "mount_context = dataset.mount(\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{\n",
              "    \"databricks\": {\n",
              "        \"eggLibraries\": [],\n",
              "        \"jarLibraries\": [],\n",
              "        \"mavenLibraries\": [],\n",
              "        \"pypiLibraries\": [],\n",
              "        \"rcranLibraries\": []\n",
              "    },\n",
              "    \"docker\": {\n",
              "        \"arguments\": [],\n",
              "        \"baseDockerfile\": null,\n",
              "        \"baseImage\": \"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210806.v1\",\n",
              "        \"baseImageRegistry\": {\n",
              "            \"address\": null,\n",
              "            \"password\": null,\n",
              "            \"registryIdentity\": null,\n",
              "            \"username\": null\n",
              "        },\n",
              "        \"enabled\": false,\n",
              "        \"platform\": {\n",
              "            \"architecture\": \"amd64\",\n",
              "            \"os\": \"Linux\"\n",
              "        },\n",
              "        \"sharedVolumes\": true,\n",
              "        \"shmSize\": null\n",
              "    },\n",
              "    \"environmentVariables\": {\n",
              "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
              "    },\n",
              "    \"inferencingStackVersion\": null,\n",
              "    \"name\": \"GPT_text_to_code\",\n",
              "    \"python\": {\n",
              "        \"baseCondaEnvironment\": null,\n",
              "        \"condaDependencies\": {\n",
              "            \"channels\": [\n",
              "                \"anaconda\",\n",
              "                \"conda-forge\"\n",
              "            ],\n",
              "            \"dependencies\": [\n",
              "                \"python=3.6.2\",\n",
              "                {\n",
              "                    \"pip\": [\n",
              "                        \"azureml\"\n",
              "                    ]\n",
              "                },\n",
              "                \"pip\"\n",
              "            ],\n",
              "            \"name\": \"azureml_560a58a3457bf693bd32c49beaf02df1\"\n",
              "        },\n",
              "        \"condaDependenciesFile\": null,\n",
              "        \"interpreterPath\": \"python\",\n",
              "        \"userManagedDependencies\": false\n",
              "    },\n",
              "    \"r\": null,\n",
              "    \"spark\": {\n",
              "        \"packages\": [],\n",
              "        \"precachePackages\": true,\n",
              "        \"repositories\": []\n",
              "    },\n",
              "    \"version\": \"1\"\n",
              "}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from azureml.core import Environment\n",
        "import os\n",
        "print(os.path.exists(\"../requirements.txt\"))\n",
        "env = Environment.from_pip_requirements('GPT_text_to_code', '../requirements.txt')\n",
        "env.register(ws)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['train/python_train_0.jsonl.gz',\n",
              " 'train/python_train_1.jsonl.gz',\n",
              " 'train/python_train_10.jsonl.gz',\n",
              " 'train/python_train_11.jsonl.gz',\n",
              " 'train/python_train_12.jsonl.gz',\n",
              " 'train/python_train_13.jsonl.gz',\n",
              " 'train/python_train_2.jsonl.gz',\n",
              " 'train/python_train_3.jsonl.gz',\n",
              " 'train/python_train_4.jsonl.gz',\n",
              " 'train/python_train_5.jsonl.gz',\n",
              " 'train/python_train_6.jsonl.gz',\n",
              " 'train/python_train_7.jsonl.gz',\n",
              " 'train/python_train_8.jsonl.gz',\n",
              " 'train/python_train_9.jsonl.gz']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[os.path.join('train', filename) for filename in os.listdir(\"../data/train\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import ComputeTarget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table style=\"width:100%\"><tr><th>Name</th><th>Workspace</th><th>State</th><th>Location</th><th>VmSize</th><th>Application URI</th><th>Docs</th></tr><tr><td><a href=\"https://ml.azure.com/compute/copilotgpt/details?wsid=/subscriptions/0abb6ec5-9030-4b3f-af04-09183c688576/resourcegroups/mltorch/workspaces/mltorch-workspace\" target=\"_blank\" rel=\"noopener\">copilotgpt</a></td><td><a href=\"https://ml.azure.com?wsid=/subscriptions/0abb6ec5-9030-4b3f-af04-09183c688576/resourcegroups/mltorch/workspaces/mltorch-workspace&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">mltorch-workspace</a></td><td>Running</td><td>westeurope</td><td>STANDARD_NC6</td><td><a href=\"https://copilotgpt.westeurope.instances.azureml.ms/tree/\" target=\"_blank\" rel=\"noopener\">Jupyter</a>  <a href=\"https://copilotgpt.westeurope.instances.azureml.ms/lab\" target=\"_blank\" rel=\"noopener\">JupyterLab</a>  <a href=\"https://copilotgpt-8787.westeurope.instances.azureml.ms\" target=\"_blank\" rel=\"noopener\">RStudio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.computeinstance.ComputeInstance?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Doc</a></td></tr></table>"
            ],
            "text/plain": [
              "{\n",
              "  \"id\": \"/subscriptions/0abb6ec5-9030-4b3f-af04-09183c688576/resourceGroups/mltorch/providers/Microsoft.MachineLearningServices/workspaces/mltorch-workspace/computes/copilotgpt\",\n",
              "  \"name\": \"copilotgpt\",\n",
              "  \"location\": \"westeurope\",\n",
              "  \"tags\": {},\n",
              "  \"properties\": {\n",
              "    \"description\": null,\n",
              "    \"computeType\": \"ComputeInstance\",\n",
              "    \"computeLocation\": \"westeurope\",\n",
              "    \"resourceId\": null,\n",
              "    \"provisioningErrors\": null,\n",
              "    \"provisioningState\": \"Succeeded\",\n",
              "    \"properties\": {\n",
              "      \"vmSize\": \"STANDARD_NC6\",\n",
              "      \"applications\": [\n",
              "        {\n",
              "          \"displayName\": \"Jupyter\",\n",
              "          \"endpointUri\": \"https://copilotgpt.westeurope.instances.azureml.ms/tree/\"\n",
              "        },\n",
              "        {\n",
              "          \"displayName\": \"Jupyter Lab\",\n",
              "          \"endpointUri\": \"https://copilotgpt.westeurope.instances.azureml.ms/lab\"\n",
              "        },\n",
              "        {\n",
              "          \"displayName\": \"RStudio\",\n",
              "          \"endpointUri\": \"https://copilotgpt-8787.westeurope.instances.azureml.ms\"\n",
              "        }\n",
              "      ],\n",
              "      \"connectivityEndpoints\": {\n",
              "        \"publicIpAddress\": \"20.50.231.57\",\n",
              "        \"privateIpAddress\": \"10.0.0.5\"\n",
              "      },\n",
              "      \"sshSettings\": {\n",
              "        \"sshPublicAccess\": \"Disabled\",\n",
              "        \"adminUserName\": \"azureuser\",\n",
              "        \"adminPublicKey\": null,\n",
              "        \"sshPort\": 4001\n",
              "      },\n",
              "      \"personalComputeInstanceSettings\": null,\n",
              "      \"subnet\": {\n",
              "        \"id\": null\n",
              "      },\n",
              "      \"errors\": []\n",
              "    },\n",
              "    \"status\": {\n",
              "      \"errors\": [],\n",
              "      \"creationTime\": \"2021-12-08T09:29:29.061407+00:00\",\n",
              "      \"createdBy\": {\n",
              "        \"userObjectId\": \"abab15fc-83d4-4460-8626-02299d460f82\",\n",
              "        \"userTenantId\": \"72f988bf-86f1-41af-91ab-2d7cd011db47\",\n",
              "        \"userName\": null\n",
              "      },\n",
              "      \"modifiedTime\": \"2021-12-13T09:00:10.000249+00:00\",\n",
              "      \"state\": \"Running\",\n",
              "      \"vmSize\": \"STANDARD_NC6\"\n",
              "    }\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_target = ws.compute_targets[\"copilotgpt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name AzureML-Triton\n",
            "Name AzureML-tensorflow-2.4-ubuntu18.04-py37-cuda11-gpu\n",
            "Name AzureML-pytorch-1.7-ubuntu18.04-py37-cuda11-gpu\n",
            "Name AzureML-pytorch-1.7-ubuntu18.04-py37-cpu-inference\n",
            "Name AzureML-minimal-ubuntu18.04-py37-cpu-inference\n",
            "Name AzureML-sklearn-0.24.1-ubuntu18.04-py37-cpu-inference\n",
            "Name AzureML-tensorflow-2.4-ubuntu18.04-py37-cpu-inference\n",
            "Name AzureML-onnxruntime-1.6-ubuntu18.04-py37-cpu-inference\n",
            "Name AzureML-tensorflow-1.15-ubuntu18.04-py37-cpu-inference\n",
            "Name AzureML-tensorflow-2.4-ubuntu18.04-py37-cuda11.0.3-gpu-inference\n",
            "Name AzureML-pytorch-1.6-ubuntu18.04-py37-cpu-inference\n",
            "Name AzureML-xgboost-0.9-ubuntu18.04-py37-cpu-inference\n",
            "Name AzureML-PyTorch-1.3-CPU\n",
            "Name AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\n",
            "Name AzureML-lightgbm-3.2-ubuntu18.04-py37-cpu\n",
            "Name AzureML-pytorch-1.8-ubuntu18.04-py37-cuda11-gpu\n",
            "Name AzureML-pytorch-1.10-ubuntu18.04-py38-cuda11-gpu\n",
            "Name AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu\n",
            "Name AzureML-mlflow-ubuntu18.04-py37-cpu-inference\n",
            "Name AzureML-VowpalWabbit-8.8.0\n"
          ]
        }
      ],
      "source": [
        "envs = Environment.list(workspace=ws)\n",
        "\n",
        "for env in envs:\n",
        "    if env.startswith(\"AzureML\"):\n",
        "        print(\"Name\",env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              "  \"name\": \"aicoplit_datastore\",\n",
              "  \"container_name\": \"data\",\n",
              "  \"account_name\": \"aicopilot\",\n",
              "  \"protocol\": \"https\",\n",
              "  \"endpoint\": \"core.windows.net\"\n",
              "}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from azureml.core import Workspace, Datastore, Dataset, Environment, Experiment\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "\n",
        "\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "# datastore = Datastore.get(ws, 'aicopilot_datastore')\n",
        "ws.datastores[\"aicoplit_datastore\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs(\"./test/folder/file.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'123/folder/test.txt'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"{id}/folder/{output_file}\".format(id=\"123\", output_file=\"test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs(\"./mytest/folder/file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.path.isdir(\"./mytest/folder/file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code CDF27G3PC to authenticate.\n",
            "\n",
            "INFO: Logging in under the \"Common\" tenant. This will log the account in under its home tenant.\n",
            "INFO: If you plan to use AzCopy with a B2B account (where the account's home tenant is separate from the tenant of the target storage account), please sign in under the target tenant with --tenant-id\n",
            "INFO: azcopy: A newer version 10.13.0 is available to download\n",
            "\n",
            "INFO: Login succeeded.\n",
            "INFO: azcopy: A newer version 10.13.0 is available to download\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!azcopy login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Scanning...\n",
            "INFO: Any empty folders will not be processed, because source and/or destination doesn't have full folder support\n",
            "\n",
            "Job 4b1bd5fb-7ca6-7c48-7545-7299f3449dce has started\n",
            "Log file is located at: /home/azureuser/.azcopy/4b1bd5fb-7ca6-7c48-7545-7299f3449dce.log\n",
            "\n",
            "INFO: azcopy: A newer version 10.13.0 is available to download\n",
            "\n",
            "0.0 %, 0 Done, 0 Failed, 3 Pending, 0 Skipped, 3 Total, \n",
            "\n",
            "\n",
            "Job 4b1bd5fb-7ca6-7c48-7545-7299f3449dce summary\n",
            "Elapsed Time (Minutes): 0.0334\n",
            "Number of File Transfers: 3\n",
            "Number of Folder Property Transfers: 0\n",
            "Total Number of Transfers: 3\n",
            "Number of Transfers Completed: 3\n",
            "Number of Transfers Failed: 0\n",
            "Number of Transfers Skipped: 0\n",
            "TotalBytesTransferred: 65145666\n",
            "Final Job Status: Completed\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!azcopy cp \"https://aicopilot.blob.core.windows.net/pipelines/20211214091552?sp=racwdlmeop&st=2021-12-14T09:25:38Z&se=2021-12-14T17:25:38Z&spr=https&sv=2020-08-04&sr=d&sig=RVHhuvBRTi6qRDec8xp19JPQcymnJwQ%2F2H0wxP0E888%3D&sdd=1\" \"./pipeline_artifacts\" --recursive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "from trainer.task import TextToCodeDataset, Trainer, TrainerConfig\n",
        "from trainer.preprocessing import Tokenizer\n",
        "from trainer.model import GPT, GPT2Config\n",
        "transform_dir = \"/home/azureuser/cloudfiles/code/Users/data/pipeline_artifacts/20211214091552/transform\"\n",
        "serving_dir = \"/home/azureuser/cloudfiles/code/Users/data/pipeline_artifacts/20211214091552/serving_dir\"\n",
        "\n",
        "tokenizer_filepath = os.path.join(transform_dir, 'transform_fn', 'tokenizer')\n",
        "ckpt_path = os.path.join(serving_dir, 'GPT.pt')\n",
        "\n",
        "tokenizer = Tokenizer.load_tokenizer(filepath=tokenizer_filepath)\n",
        "sequence_length = 128\n",
        "train_dataset = TextToCodeDataset(transform_dir, sequence_length)\n",
        "val_dataset = TextToCodeDataset(transform_dir, sequence_length)\n",
        "mconf = GPT2Config(\n",
        "    tokenizer.vocab_size,\n",
        "    embedding_size=768,\n",
        "    n_heads=12,\n",
        "    n_layers=3,\n",
        "    max_sequence_length=sequence_length\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 0/24 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "epoch 1 iter 23: train loss 0.48336. lr 3.564375e-04: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:08<00:00,  3.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 1 iter 23: train loss 0.48336. lr 3.564375e-04: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:09<00:00,  2.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from trainer.task import TextToCodeDataset, Trainer, TrainerConfig\n",
        "from trainer.model import GPT, GPT2Config\n",
        "model = GPT(mconf)\n",
        "tconf = TrainerConfig(\n",
        "    max_epochs=1,\n",
        "    batch_size=16,\n",
        "    learning_rate=6e-4,\n",
        "    lr_decay=True,\n",
        "    warmup_tokens=512*20,\n",
        "    final_tokens=2*len(train_dataset)*sequence_length,\n",
        "    num_workers=4,\n",
        "    ckpt_path=ckpt_path\n",
        ")\n",
        "trainer = Trainer(model, train_dataset, val_dataset, tconf)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[332, 14179, 64, 7637, 9, 366, 317, 318, 1579, 16434, 336, 298, 10584, 305]\n",
            "torch.Size([1, 14])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-12-14 13:03:03.956941: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/mic/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/intel64/lib:/opt/intel/compilers_and_libraries_2018.3.222/linux/mpi/mic/lib::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n",
            "2021-12-14 13:03:03.956992: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'def say_hello(name): \"\"\"print hello to the person\"\"\"<bos> \\n    if not pages_settings.PAGE_CONTENT_CONTENT_REVISION:\\n        return {\\'revisions\\': None}\\n    revisions = Content.filter(page, language=lang,\\n    return {\\'revisions\\': None}\\n    return {\\'revisions\\': None}\\n    return {\\'revisions\\': revisions = Content.objects.filter(page,\\n    return {\\'revisions\\': revisions = Content.filter(page,\\n    return {\\'revisions\\': revisions = Content.filter(page,\\n    return {\\'revisions\\': revisions = Content.filter(page,\\n    return {\\'revisions\\': revisions = Content.filter(page=lang,\\n    return {\\'revisions\\': revisions[0:10]}<eos>\\n    \"\"\"\\n    \"\"\"\\ndef do_videoplaceholder(parser, token):\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n    \"\"\"\\n'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "tokenizer = Tokenizer.load_tokenizer(filepath=tokenizer_filepath)\n",
        "context = 'def say_hello(name): \"\"\"print hello to the person\"\"\"'\n",
        "tokens_idx = tokenizer.encode(context)\n",
        "print(tokens_idx)\n",
        "x = torch.tensor(tokens_idx, dtype=torch.long)[None,...].to(trainer.device)\n",
        "print(x.shape)\n",
        "model(x)\n",
        "# x = torch.tensor([train_dataset.char2idx[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "y = utils.sample(model, x, 250, temperature=1.0, sample=False, top_k=10)[0]\n",
        "tokenizer.decode(y)\n",
        "# completion = ''.join([train_dataset.idx2char[int(i)] for i in y])\n",
        "# print(completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = os.path.join(serving_dir, \"GPT.pt\")\n",
        "\n",
        "loaded_model = torch.load(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.0045,  1.5744, -2.2895,  ..., -1.8127, -1.6139, -2.6518],\n",
              "         [ 0.9025,  3.3443, -3.1073,  ..., -3.4499, -2.1364, -3.5163],\n",
              "         [ 1.2823,  1.9669, -3.9924,  ..., -4.4982, -2.6209, -4.6499],\n",
              "         ...,\n",
              "         [ 0.7448,  0.9686, -2.4242,  ..., -3.8466, -1.5266, -3.8590],\n",
              "         [ 0.5264,  2.1520, -2.9087,  ..., -3.6783, -2.6302, -3.1395],\n",
              "         [14.1495,  1.5194, -2.7489,  ..., -2.8840, -1.8600, -3.7565]]],\n",
              "       device='cuda:0', grad_fn=<UnsafeViewBackward>)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_loaded = GPT(mconf)\n",
        "model_loaded.load_state_dict(loaded_model)\n",
        "model_loaded.eval()\n",
        "model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (128) must match the size of tensor b (20) at non-singleton dimension 3",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_32783/1740258026.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/trainer/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mpositional_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embedding\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpositional_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/trainer/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/copilotgpt/code/Users/stefruinard/ai_copilot/trainer/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nqhd,nkhd->nhqk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcausal_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-1e20'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (20) at non-singleton dimension 3"
          ]
        }
      ],
      "source": [
        "x = torch.ones((1, 20), dtype=torch.long).to(trainer.device)\n",
        "model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "74f09c4dcce712dedd57a93287389ba50a94d19f06041f2f1e451d0a71cf11ad"
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
